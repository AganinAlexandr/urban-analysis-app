#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
"""
import sqlite3
import os
import re
from collections import Counter, defaultdict
from datetime import datetime

def clean_text(text):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if not text:
        return ""
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = text.lower()
    
    # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ü–∏—Ñ—Ä—ã
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\d+', ' ', text)
    
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def extract_words(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return []
    
    cleaned_text = clean_text(text)
    words = cleaned_text.split()
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (–º–µ–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤)
    words = [word for word in words if len(word) >= 3]
    
    return words

def get_group_data():
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –∏–∑ –ë–î"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return {}
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã —Å –∏—Ö –≥—Ä—É–ø–ø–∞–º–∏ –∏ –æ—Ç–∑—ã–≤–∞–º–∏
        cursor.execute("""
            SELECT 
                og.group_type,
                o.name,
                GROUP_CONCAT(r.review_text, ' | ') as all_reviews
            FROM objects o
            LEFT JOIN object_groups og ON o.group_id = og.id
            LEFT JOIN reviews r ON o.id = r.object_id
            WHERE og.group_type IS NOT NULL
            GROUP BY og.group_type, o.name
        """)
        
        group_data = defaultdict(lambda: {'names': [], 'reviews': []})
        
        for group_type, name, reviews_text in cursor.fetchall():
            if group_type:
                group_data[group_type]['names'].append(name)
                if reviews_text:
                    group_data[group_type]['reviews'].append(reviews_text)
        
        conn.close()
        return group_data
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
        return {}

def analyze_keywords():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã"""
    print("üîç –ê–ù–ê–õ–ò–ó –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í")
    print("=" * 50)
    
    group_data = get_group_data()
    
    if not group_data:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return {}
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
    group_words = {}
    all_words = Counter()
    
    for group_type, data in group_data.items():
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑ –≥—Ä—É–ø–ø—ã: {group_type}")
        print(f"   –û–±—ä–µ–∫—Ç–æ–≤: {len(data['names'])}")
        print(f"   –û—Ç–∑—ã–≤–æ–≤: {len(data['reviews'])}")
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏–π –æ–±—ä–µ–∫—Ç–æ–≤
        name_words = []
        for name in data['names']:
            name_words.extend(extract_words(name))
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –æ—Ç–∑—ã–≤–æ–≤
        review_words = []
        for review in data['reviews']:
            review_words.extend(extract_words(review))
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤
        name_counter = Counter(name_words)
        review_counter = Counter(review_words)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –≥—Ä—É–ø–ø—ã
        group_counter = name_counter + review_counter
        group_words[group_type] = group_counter
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å—á–µ—Ç—á–∏–∫
        all_words.update(group_counter)
        
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(group_counter)}")
        print(f"   –í—Å–µ–≥–æ —Å–ª–æ–≤: {sum(group_counter.values())}")
    
    return group_words, all_words

def find_unique_keywords(group_words, all_words, min_frequency=2):
    """–ù–∞—Ö–æ–¥–∏—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã"""
    print("\nüéØ –ü–û–ò–°–ö –£–ù–ò–ö–ê–õ–¨–ù–´–• –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í")
    print("=" * 50)
    
    unique_keywords = {}
    
    for group_type, group_counter in group_words.items():
        print(f"\nüìÑ –ì—Ä—É–ø–ø–∞: {group_type}")
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ —ç—Ç–æ–π –≥—Ä—É–ø–ø–µ
        unique_words = []
        
        for word, count in group_counter.most_common():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —á–∞—Å—Ç–æ—Ç—É
            if count < min_frequency:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ª–æ–≤–æ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –¥—Ä—É–≥–∏—Ö –≥—Ä—É–ø–ø–∞—Ö
            is_unique = True
            for other_group, other_counter in group_words.items():
                if other_group != group_type and word in other_counter:
                    is_unique = False
                    break
            
            if is_unique:
                unique_words.append((word, count))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        unique_words.sort(key=lambda x: x[1], reverse=True)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø-10 —Å–ª–æ–≤
        top_words = unique_words[:10]
        
        unique_keywords[group_type] = top_words
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(unique_words)}")
        print(f"   –¢–æ–ø-10: {[word for word, count in top_words]}")
    
    return unique_keywords

def create_keywords_mapping(unique_keywords):
    """–°–æ–∑–¥–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ë–î"""
    keywords_mapping = {}
    
    for group_type, words in unique_keywords.items():
        keywords_mapping[group_type] = {
            'name_keywords': [word for word, count in words if count >= 3],  # –ë–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π
            'text_keywords': [word for word, count in words],  # –í—Å–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
            'negative_keywords': []  # –ü–æ–∫–∞ –ø—É—Å—Ç–æ–π, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∑–∂–µ
        }
    
    return keywords_mapping

def save_keywords_to_db(keywords_mapping):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –ë–î"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS group_keywords (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                group_type TEXT NOT NULL,
                keyword_type TEXT NOT NULL,
                keyword TEXT NOT NULL,
                frequency INTEGER DEFAULT 1,
                weight REAL DEFAULT 1.0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(group_type, keyword_type, keyword)
            )
        """)
        
        # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        cursor.execute("DELETE FROM group_keywords")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        added_count = 0
        for group_type, keywords in keywords_mapping.items():
            for keyword_type, words in keywords.items():
                for word in words:
                    cursor.execute("""
                        INSERT INTO group_keywords (group_type, keyword_type, keyword, weight)
                        VALUES (?, ?, ?, ?)
                    """, (group_type, keyword_type, word, 1.0))
                    added_count += 1
        
        conn.commit()
        conn.close()
        
        print(f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {added_count}")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return False

def show_keywords_summary(keywords_mapping):
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    print("\nüìã –°–í–û–î–ö–ê –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í")
    print("=" * 50)
    
    for group_type, keywords in keywords_mapping.items():
        print(f"\nüè• {group_type.upper()}:")
        
        name_count = len(keywords['name_keywords'])
        text_count = len(keywords['text_keywords'])
        
        print(f"   ‚Ä¢ –ù–∞–∑–≤–∞–Ω–∏—è: {name_count} —Å–ª–æ–≤")
        if name_count > 0:
            print(f"     –ü—Ä–∏–º–µ—Ä—ã: {', '.join(keywords['name_keywords'][:5])}")
        
        print(f"   ‚Ä¢ –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–æ–≤: {text_count} —Å–ª–æ–≤")
        if text_count > 0:
            print(f"     –ü—Ä–∏–º–µ—Ä—ã: {', '.join(keywords['text_keywords'][:5])}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=== –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í ===")
    print(f"‚è∞ –í—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    group_words, all_words = analyze_keywords()
    
    if not group_words:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    # –ù–∞—Ö–æ–¥–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    unique_keywords = find_unique_keywords(group_words, all_words, min_frequency=2)
    
    if not unique_keywords:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        return
    
    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥
    keywords_mapping = create_keywords_mapping(unique_keywords)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–≤–æ–¥–∫—É
    show_keywords_summary(keywords_mapping)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
    if save_keywords_to_db(keywords_mapping):
        print("‚úÖ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    
    print("\nüéâ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == "__main__":
    main() 