#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π
"""
import sqlite3
import os
import re
import pymorphy2
from collections import Counter, defaultdict
from datetime import datetime

class EnhancedKeywordProcessor:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π"""
    
    def __init__(self):
        self.morph = pymorphy2.MorphAnalyzer()
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.normalization_dict = {
            '–±–æ–ª—å–Ω–∏—Ü–∞': '–±–æ–ª—å–Ω–∏—Ü–∞',
            '–≥–æ—Å–ø–∏—Ç–∞–ª—å': '–±–æ–ª—å–Ω–∏—Ü–∞',
            '–º–µ–¥—Ü–µ–Ω—Ç—Ä': '–±–æ–ª—å–Ω–∏—Ü–∞',
            '–∫–ª–∏–Ω–∏–∫–∞': '–±–æ–ª—å–Ω–∏—Ü–∞',
            '–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞': '–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞',
            '–∞–º–±—É–ª–∞—Ç–æ—Ä–∏—è': '–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞',
            '—à–∫–æ–ª–∞': '—à–∫–æ–ª–∞',
            '–ª–∏—Ü–µ–π': '—à–∫–æ–ª–∞',
            '–≥–∏–º–Ω–∞–∑–∏—è': '—à–∫–æ–ª–∞',
            '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç': '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç',
            '–∏–Ω—Å—Ç–∏—Ç—É—Ç': '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç',
            '–∞–∫–∞–¥–µ–º–∏—è': '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç',
            '–≤—É–∑': '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç',
            '–∞–ø—Ç–µ–∫–∞': '–∞–ø—Ç–µ–∫–∞',
            '—Ñ–∞—Ä–º–∞—Ü–∏—è': '–∞–ø—Ç–µ–∫–∞',
            '–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥': '–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥',
            '—Å–∞–¥': '–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥',
            '–¥–µ—Ç—Å–∞–¥': '–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥',
            '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä': '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä',
            '–º–æ–ª–ª': '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä',
            '–≥–∞–ª–µ—Ä–µ—è': '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä'
        }
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        self.stop_words = {
            '—ç—Ç–æ', '—Ç–æ—Ç', '—Ç–∞–∫–æ–π', '–∫–∞–∫–æ–π', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫–∞–∫', '—á—Ç–æ', '–∫—Ç–æ',
            '–æ—á–µ–Ω—å', '–º–Ω–æ–≥–æ', '–º–∞–ª–æ', '–±–æ–ª—å—à–µ', '–º–µ–Ω—å—à–µ', '–≤—Å–µ', '–≤—Å–µ–≥–¥–∞', '–Ω–∏–∫–æ–≥–¥–∞',
            '—Ö–æ—Ä–æ—à–æ', '–ø–ª–æ—Ö–æ', '–ª—É—á—à–µ', '—Ö—É–∂–µ', '–Ω–æ–≤—ã–π', '—Å—Ç–∞—Ä—ã–π', '–±–æ–ª—å—à–æ–π', '–º–∞–ª–µ–Ω—å–∫–∏–π'
        }
    
    def normalize_word(self, word):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ª–æ–≤–æ, –ø—Ä–∏–≤–æ–¥—è –∫ –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º–µ"""
        if not word:
            return ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        word = word.lower().strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–æ–≤–∞—Ä—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        if word in self.normalization_dict:
            return self.normalization_dict[word]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º pymorphy2 –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏
        try:
            parsed = self.morph.parse(word)
            if parsed:
                # –ë–µ—Ä–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É (–∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤)
                return parsed[0].normal_form
        except:
            pass
        
        return word
    
    def clean_text(self, text):
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not text:
            return ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ü–∏—Ñ—Ä—ã
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_normalized_words(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return []
        
        cleaned_text = self.clean_text(text)
        words = cleaned_text.split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        normalized_words = []
        for word in words:
            if len(word) >= 3 and word not in self.stop_words:
                normalized_word = self.normalize_word(word)
                if normalized_word and len(normalized_word) >= 3:
                    normalized_words.append(normalized_word)
        
        return normalized_words

def create_enhanced_keywords_table():
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS enhanced_group_keywords (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                group_type TEXT NOT NULL,
                keyword_type TEXT NOT NULL,
                original_keyword TEXT NOT NULL,
                normalized_keyword TEXT NOT NULL,
                frequency INTEGER DEFAULT 1,
                weight REAL DEFAULT 1.0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(group_type, keyword_type, normalized_keyword)
            )
        """)
        
        # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        cursor.execute("DELETE FROM enhanced_group_keywords")
        
        conn.commit()
        conn.close()
        
        print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å–æ–∑–¥–∞–Ω–∞")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã: {e}")
        return False

def analyze_enhanced_keywords():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
    print("üîç –ê–ù–ê–õ–ò–ó –£–õ–£–ß–®–ï–ù–ù–´–• –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í")
    print("=" * 50)
    
    processor = EnhancedKeywordProcessor()
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return {}
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã —Å –∏—Ö –≥—Ä—É–ø–ø–∞–º–∏ –∏ –æ—Ç–∑—ã–≤–∞–º–∏
        cursor.execute("""
            SELECT 
                og.group_type,
                o.name,
                GROUP_CONCAT(r.review_text, ' | ') as all_reviews
            FROM objects o
            LEFT JOIN object_groups og ON o.group_id = og.id
            LEFT JOIN reviews r ON o.id = r.object_id
            WHERE og.group_type IS NOT NULL
            GROUP BY og.group_type, o.name
        """)
        
        group_data = defaultdict(lambda: {'names': [], 'reviews': []})
        
        for group_type, name, reviews_text in cursor.fetchall():
            if group_type:
                group_data[group_type]['names'].append(name)
                if reviews_text:
                    group_data[group_type]['reviews'].append(reviews_text)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        group_words = {}
        all_words = Counter()
        
        for group_type, data in group_data.items():
            print(f"\nüìä –ê–Ω–∞–ª–∏–∑ –≥—Ä—É–ø–ø—ã: {group_type}")
            print(f"   –û–±—ä–µ–∫—Ç–æ–≤: {len(data['names'])}")
            print(f"   –û—Ç–∑—ã–≤–æ–≤: {len(data['reviews'])}")
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏–π –æ–±—ä–µ–∫—Ç–æ–≤
            name_words = []
            for name in data['names']:
                name_words.extend(processor.extract_normalized_words(name))
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –æ—Ç–∑—ã–≤–æ–≤
            review_words = []
            for review in data['reviews']:
                review_words.extend(processor.extract_normalized_words(review))
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤
            name_counter = Counter(name_words)
            review_counter = Counter(review_words)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –≥—Ä—É–ø–ø—ã
            group_counter = name_counter + review_counter
            group_words[group_type] = group_counter
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å—á–µ—Ç—á–∏–∫
            all_words.update(group_counter)
            
            print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(group_counter)}")
            print(f"   –í—Å–µ–≥–æ —Å–ª–æ–≤: {sum(group_counter.values())}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 —Å–ª–æ–≤
            top_words = group_counter.most_common(5)
            print(f"   –¢–æ–ø-5 —Å–ª–æ–≤: {[word for word, count in top_words]}")
        
        conn.close()
        return group_words, all_words
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
        return {}

def find_enhanced_unique_keywords(group_words, all_words, min_frequency=2):
    """–ù–∞—Ö–æ–¥–∏—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
    print("\nüéØ –ü–û–ò–°–ö –£–ù–ò–ö–ê–õ–¨–ù–´–• –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í (–° –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ï–ô)")
    print("=" * 50)
    
    unique_keywords = {}
    
    for group_type, group_counter in group_words.items():
        print(f"\nüìÑ –ì—Ä—É–ø–ø–∞: {group_type}")
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ —ç—Ç–æ–π –≥—Ä—É–ø–ø–µ
        unique_words = []
        
        for word, count in group_counter.most_common():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —á–∞—Å—Ç–æ—Ç—É
            if count < min_frequency:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ª–æ–≤–æ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –¥—Ä—É–≥–∏—Ö –≥—Ä—É–ø–ø–∞—Ö
            is_unique = True
            for other_group, other_counter in group_words.items():
                if other_group != group_type and word in other_counter:
                    is_unique = False
                    break
            
            if is_unique:
                unique_words.append((word, count))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        unique_words.sort(key=lambda x: x[1], reverse=True)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø-10 —Å–ª–æ–≤
        top_words = unique_words[:10]
        
        unique_keywords[group_type] = top_words
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(unique_words)}")
        print(f"   –¢–æ–ø-10: {[word for word, count in top_words]}")
    
    return unique_keywords

def save_enhanced_keywords_to_db(unique_keywords):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –ë–î"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False
    
    processor = EnhancedKeywordProcessor()
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        cursor.execute("DELETE FROM enhanced_group_keywords")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        added_count = 0
        for group_type, words in unique_keywords.items():
            for word, count in words:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ª–æ–≤–æ
                normalized_word = processor.normalize_word(word)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
                keyword_type = 'name' if count >= 3 else 'text'
                
                cursor.execute("""
                    INSERT INTO enhanced_group_keywords 
                    (group_type, keyword_type, original_keyword, normalized_keyword, frequency, weight)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (group_type, keyword_type, word, normalized_word, count, 1.0))
                added_count += 1
        
        conn.commit()
        conn.close()
        
        print(f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {added_count}")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return False

def detect_group_by_enhanced_keywords(object_name, review_text):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä—É–ø–ø—É –æ–±—ä–µ–∫—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        return 'undetected', 0.0
    
    processor = EnhancedKeywordProcessor()
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        cursor.execute("""
            SELECT group_type, keyword_type, normalized_keyword, weight 
            FROM enhanced_group_keywords 
            ORDER BY group_type, keyword_type
        """)
        keywords = cursor.fetchall()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        normalized_object_name = ' '.join(processor.extract_normalized_words(object_name or ""))
        normalized_review_text = ' '.join(processor.extract_normalized_words(review_text or ""))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º
        group_scores = {}
        
        for group_type, keyword_type, normalized_keyword, weight in keywords:
            if group_type not in group_scores:
                group_scores[group_type] = {'name_score': 0, 'text_score': 0, 'negative_score': 0}
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞
            if keyword_type == 'name' and normalized_object_name:
                if normalized_keyword in normalized_object_name:
                    group_scores[group_type]['name_score'] += weight
                    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏: '{normalized_keyword}' –≤ '{normalized_object_name}'")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ –æ—Ç–∑—ã–≤–∞
            if keyword_type == 'text' and normalized_review_text:
                if normalized_keyword in normalized_review_text:
                    group_scores[group_type]['text_score'] += weight
                    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ –æ—Ç–∑—ã–≤–µ: '{normalized_keyword}' –≤ '{normalized_review_text}'")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        best_group = 'undetected'
        best_score = 0.0
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—Å—á–µ—Ç–∞ –±–∞–ª–ª–æ–≤:")
        for group_type, scores in group_scores.items():
            # –§–æ—Ä–º—É–ª–∞: (name_score * 2) + text_score - negative_score
            total_score = (scores['name_score'] * 2) + scores['text_score'] - scores['negative_score']
            
            print(f"   {group_type}: name={scores['name_score']}, text={scores['text_score']}, negative={scores['negative_score']}, total={total_score}")
            
            if total_score > best_score:
                best_score = total_score
                best_group = group_type
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0.0 - 1.0)
        confidence = min(best_score / 5.0, 1.0) if best_score > 0 else 0.0
        
        conn.close()
        return best_group, confidence
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä—É–ø–ø—ã: {e}")
        return 'undetected', 0.0

def test_enhanced_system():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É"""
    print("=== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–û–ô –°–ò–°–¢–ï–ú–´ ===")
    print(f"‚è∞ –í—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    test_cases = [
        ("–ì–æ—Ä–æ–¥—Å–∫–∞—è –±–æ–ª—å–Ω–∏—Ü–∞ ‚Ññ1", "–û—Ç–ª–∏—á–Ω–∞—è –±–æ–ª—å–Ω–∏—Ü–∞ —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –≤—Ä–∞—á–∞–º–∏"),
        ("–ê–ø—Ç–µ–∫–∞ –Ω–∞ —É–≥–ª—É", "–£–¥–æ–±–Ω–∞—è –∞–ø—Ç–µ–∫–∞, –≤—Å–µ–≥–¥–∞ –µ—Å—Ç—å –Ω—É–∂–Ω—ã–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞"),
        ("–®–∫–æ–ª–∞ ‚Ññ15", "–•–æ—Ä–æ—à–∞—è —à–∫–æ–ª–∞, –Ω–æ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–∏—Ç–∞–Ω–∏–µ–º"),
        ("–¢–µ—Å—Ç–æ–≤–∞—è –±–æ–ª—å–Ω–∏—Ü–∞", "–û—Ç–ª–∏—á–Ω–∞—è –±–æ–ª—å–Ω–∏—Ü–∞"),
        ("–¢–µ—Å—Ç–æ–≤–∞—è —à–∫–æ–ª–∞", "–•–æ—Ä–æ—à–∞—è —à–∫–æ–ª–∞"),
        ("–¢–µ—Å—Ç–æ–≤–∞—è –∞–ø—Ç–µ–∫–∞", "–£–¥–æ–±–Ω–∞—è –∞–ø—Ç–µ–∫–∞")
    ]
    
    for i, (object_name, review_text) in enumerate(test_cases, 1):
        print(f"\nüß™ –¢–µ—Å—Ç {i}:")
        print(f"üìÑ –û–±—ä–µ–∫—Ç: '{object_name}'")
        print(f"üìù –û—Ç–∑—ã–≤: '{review_text}'")
        
        detected_group, confidence = detect_group_by_enhanced_keywords(object_name, review_text)
        
        print(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç: {detected_group} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
        print("-" * 50)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í ===")
    print(f"‚è∞ –í—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤...")
    if not create_enhanced_keywords_table():
        print("‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã")
        return
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    group_words, all_words = analyze_enhanced_keywords()
    
    if not group_words:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    # –ù–∞—Ö–æ–¥–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    unique_keywords = find_enhanced_unique_keywords(group_words, all_words, min_frequency=2)
    
    if not unique_keywords:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        return
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
    if save_enhanced_keywords_to_db(unique_keywords):
        print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã...")
    test_enhanced_system()
    
    print("\nüéâ –£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞!")

if __name__ == "__main__":
    main() 