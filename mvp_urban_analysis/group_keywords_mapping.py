#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä—É–ø–ø –æ–±—ä–µ–∫—Ç–æ–≤
"""
import sqlite3
import os
from datetime import datetime

# –°–ª–æ–≤–∞—Ä—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
GROUP_KEYWORDS = {
    'hospitals': {
        'name_keywords': ['–±–æ–ª—å–Ω–∏—Ü–∞', '–≥–æ—Å–ø–∏—Ç–∞–ª—å', '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä', '–∫–ª–∏–Ω–∏–∫–∞', '–º–µ–¥—Ü–µ–Ω—Ç—Ä', '–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞'],
        'text_keywords': ['–≤—Ä–∞—á', '–ª–µ—á–µ–Ω–∏–µ', '–ø–∞—Ü–∏–µ–Ω—Ç', '–º–µ–¥–∏—Ü–∏–Ω–∞', '–∑–¥–æ—Ä–æ–≤—å–µ', '–±–æ–ª–µ–∑–Ω—å', '—Å–∏–º–ø—Ç–æ–º', '–¥–∏–∞–≥–Ω–æ–∑', '–æ–ø–µ—Ä–∞—Ü–∏—è', '—Ç–µ—Ä–∞–ø–∏—è'],
        'negative_keywords': ['–æ—á–µ—Ä–µ–¥—å', '–∑–∞–ø–∏—Å—å', '–ø—Ä–∏–µ–º', '–∞–Ω–∞–ª–∏–∑', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ']
    },
    'schools': {
        'name_keywords': ['—à–∫–æ–ª–∞', '–ª–∏—Ü–µ–π', '–≥–∏–º–Ω–∞–∑–∏—è', '–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä', '—É—á–µ–±–Ω–æ–µ –∑–∞–≤–µ–¥–µ–Ω–∏–µ'],
        'text_keywords': ['—É—á–∏—Ç–µ–ª—å', '—É—á–µ–Ω–∏–∫', '—É—Ä–æ–∫', '–∫–ª–∞—Å—Å', '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–æ–±—É—á–µ–Ω–∏–µ', '–¥–∏—Ä–µ–∫—Ç–æ—Ä', '–∑–∞–≤—É—á', '–ø—Ä–µ–¥–º–µ—Ç', '—ç–∫–∑–∞–º–µ–Ω'],
        'negative_keywords': ['–¥–æ–º–∞—à–∫–∞', '–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è', '–æ—Ü–µ–Ω–∫–∞', '–¥–≤–æ–π–∫–∞', '—Ç—Ä–æ–π–∫–∞', '—á–µ—Ç–≤–µ—Ä–∫–∞', '–ø—è—Ç–µ—Ä–∫–∞']
    },
    'kindergartens': {
        'name_keywords': ['–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥', '—Å–∞–¥', '–¥–æ—à–∫–æ–ª—å–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ', '—è—Å–ª–∏', '–¥–µ—Ç—Å–∞–¥'],
        'text_keywords': ['–≤–æ—Å–ø–∏—Ç–∞—Ç–µ–ª—å', '—Ä–µ–±–µ–Ω–æ–∫', '–≥—Ä—É–ø–ø–∞', '–∏–≥—Ä–∞', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '–∑–∞–Ω—è—Ç–∏–µ', '–ø—Ä–æ–≥—É–ª–∫–∞', '—Å–æ–Ω', '–µ–¥–∞', '–∞–¥–∞–ø—Ç–∞—Ü–∏—è'],
        'negative_keywords': ['–ø–ª–∞—á', '–∫–∞–ø—Ä–∏–∑', '–Ω–µ —Ö–æ—á—É', '–Ω–µ –±—É–¥—É', '—Å—Ç—Ä–∞—à–Ω–æ', '–±–æ—é—Å—å']
    },
    'polyclinics': {
        'name_keywords': ['–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞', '–∞–º–±—É–ª–∞—Ç–æ—Ä–∏—è', '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', '–¥–∏—Å–ø–∞–Ω—Å–µ—Ä'],
        'text_keywords': ['—Ç–µ—Ä–∞–ø–µ–≤—Ç', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', '–æ—Å–º–æ—Ç—Ä', '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '—Å–ø—Ä–∞–≤–∫–∞', '–±–æ–ª—å–Ω–∏—á–Ω—ã–π', '—Ä–µ—Ü–µ–ø—Ç'],
        'negative_keywords': ['–æ—á–µ—Ä–µ–¥—å', '–∑–∞–ø–∏—Å—å', '–ø—Ä–∏–µ–º', '–∞–Ω–∞–ª–∏–∑', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ']
    },
    'pharmacies': {
        'name_keywords': ['–∞–ø—Ç–µ–∫–∞', '—Ñ–∞—Ä–º–∞—Ü–∏—è', '–ª–µ–∫–∞—Ä—Å—Ç–≤–æ', '–º–µ–¥–∏–∫–∞–º–µ–Ω—Ç'],
        'text_keywords': ['–ª–µ–∫–∞—Ä—Å—Ç–≤–æ', '—Ç–∞–±–ª–µ—Ç–∫–∞', '—Å–∏—Ä–æ–ø', '–º–∞–∑—å', '—Ä–µ—Ü–µ–ø—Ç', '—Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç', '–ø—Ä–æ–≤–∏–∑–æ—Ä', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–∞–Ω–∞–ª–æ–≥'],
        'negative_keywords': ['–¥–æ—Ä–æ–≥–æ', '–Ω–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏', '–∑–∞–º–µ–Ω–∏—Ç–µ–ª—å', '–ø–æ–±–æ—á–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç']
    },
    'shopping_malls': {
        'name_keywords': ['—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä', '–º–æ–ª–ª', '–≥–∞–ª–µ—Ä–µ—è', '–ø–∞—Å—Å–∞–∂', '—Ç–æ—Ä–≥–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å', '—à–æ–ø–ø–∏–Ω–≥'],
        'text_keywords': ['–º–∞–≥–∞–∑–∏–Ω', '–ø–æ–∫—É–ø–∫–∞', '—Ç–æ–≤–∞—Ä', '—Ü–µ–Ω–∞', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '–∫–∞—Å—Å–∞', '–ø—Ä–æ–¥–∞–≤–µ—Ü', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç', '—Ä–∞–∑–º–µ—Ä'],
        'negative_keywords': ['–¥–æ—Ä–æ–≥–æ', '–æ—á–µ—Ä–µ–¥—å', '–Ω–µ—Ç —Ä–∞–∑–º–µ—Ä–∞', '–Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç', '–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è']
    },
    'universities': {
        'name_keywords': ['—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '–∏–Ω—Å—Ç–∏—Ç—É—Ç', '–∞–∫–∞–¥–µ–º–∏—è', '–≤—É–∑', '–≤—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ'],
        'text_keywords': ['—Å—Ç—É–¥–µ–Ω—Ç', '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å', '–ª–µ–∫—Ü–∏—è', '—Å–µ–º–∏–Ω–∞—Ä', '—ç–∫–∑–∞–º–µ–Ω', '—Å–µ—Å—Å–∏—è', '–¥–∏–ø–ª–æ–º', '–∫–∞—Ñ–µ–¥—Ä–∞', '—Ñ–∞–∫—É–ª—å—Ç–µ—Ç', '—Ä–µ–∫—Ç–æ—Ä'],
        'negative_keywords': ['—Å–ª–æ–∂–Ω–æ', '—Ç—Ä—É–¥–Ω–æ', '–Ω–µ –ø–æ–Ω–∏–º–∞—é', '–∑–∞–≤–∞–ª–∏–ª', '–Ω–µ —Å–¥–∞–ª']
    }
}

def create_keywords_table():
    """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS group_keywords (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                group_type TEXT NOT NULL,
                keyword_type TEXT NOT NULL, -- 'name', 'text', 'negative'
                keyword TEXT NOT NULL,
                weight REAL DEFAULT 1.0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(group_type, keyword_type, keyword)
            )
        """)
        
        # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        cursor.execute("DELETE FROM group_keywords")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        added_count = 0
        for group_type, keywords in GROUP_KEYWORDS.items():
            for keyword_type, words in keywords.items():
                for word in words:
                    cursor.execute("""
                        INSERT INTO group_keywords (group_type, keyword_type, keyword, weight)
                        VALUES (?, ?, ?, ?)
                    """, (group_type, keyword_type, word, 1.0))
                    added_count += 1
        
        conn.commit()
        conn.close()
        
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {added_count}")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return False

def detect_group_by_keywords(object_name, review_text):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä—É–ø–ø—É –æ–±—ä–µ–∫—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        return 'undetected', 0.0
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        cursor.execute("""
            SELECT group_type, keyword_type, keyword, weight 
            FROM group_keywords 
            ORDER BY group_type, keyword_type
        """)
        keywords = cursor.fetchall()
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º
        group_scores = {}
        
        for group_type, keyword_type, keyword, weight in keywords:
            if group_type not in group_scores:
                group_scores[group_type] = {'name_score': 0, 'text_score': 0, 'negative_score': 0}
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞
            if keyword_type == 'name' and object_name:
                if keyword.lower() in object_name.lower():
                    group_scores[group_type]['name_score'] += weight
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ –æ—Ç–∑—ã–≤–∞
            if keyword_type == 'text' and review_text:
                if keyword.lower() in review_text.lower():
                    group_scores[group_type]['text_score'] += weight
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if keyword_type == 'negative' and review_text:
                if keyword.lower() in review_text.lower():
                    group_scores[group_type]['negative_score'] += weight
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        best_group = 'undetected'
        best_score = 0.0
        
        for group_type, scores in group_scores.items():
            # –§–æ—Ä–º—É–ª–∞: (name_score * 2) + text_score - negative_score
            total_score = (scores['name_score'] * 2) + scores['text_score'] - scores['negative_score']
            
            if total_score > best_score:
                best_score = total_score
                best_group = group_type
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0.0 - 1.0)
        confidence = min(best_score / 10.0, 1.0) if best_score > 0 else 0.0
        
        conn.close()
        return best_group, confidence
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä—É–ø–ø—ã: {e}")
        return 'undetected', 0.0

def update_detected_groups():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç detected_groups –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã —Å –∏—Ö –æ—Ç–∑—ã–≤–∞–º–∏
        cursor.execute("""
            SELECT o.id, o.name, o.group_id, 
                   GROUP_CONCAT(r.review_text, ' | ') as all_reviews
            FROM objects o
            LEFT JOIN reviews r ON o.id = r.object_id
            GROUP BY o.id, o.name, o.group_id
        """)
        objects = cursor.fetchall()
        
        updated_count = 0
        
        for obj_id, obj_name, current_group_id, reviews_text in objects:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            detected_group, confidence = detect_group_by_keywords(obj_name, reviews_text or "")
            
            # –ü–æ–ª—É—á–∞–µ–º ID –æ–ø—Ä–µ–¥–µ–ª—è–µ–º–æ–π –≥—Ä—É–ø–ø—ã
            cursor.execute("SELECT id FROM detected_groups WHERE group_type = ?", (detected_group,))
            detected_group_result = cursor.fetchone()
            
            if detected_group_result:
                detected_group_id = detected_group_result[0]
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç
                cursor.execute("""
                    UPDATE objects 
                    SET detected_group_id = ? 
                    WHERE id = ?
                """, (detected_group_id, obj_id))
                
                updated_count += 1
                print(f"   ‚úÖ –û–±—ä–µ–∫—Ç '{obj_name}': {detected_group} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
        
        conn.commit()
        conn.close()
        
        print(f"\n‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {updated_count}")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥—Ä—É–ø–ø: {e}")
        return False

def show_keywords_statistics():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í:")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º
        cursor.execute("""
            SELECT group_type, keyword_type, COUNT(*) as count
            FROM group_keywords
            GROUP BY group_type, keyword_type
            ORDER BY group_type, keyword_type
        """)
        stats = cursor.fetchall()
        
        current_group = None
        for group_type, keyword_type, count in stats:
            if current_group != group_type:
                print(f"\nüìÑ {group_type.upper()}:")
                current_group = group_type
            
            type_name = {
                'name': '–ù–∞–∑–≤–∞–Ω–∏—è',
                'text': '–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–æ–≤', 
                'negative': '–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ'
            }.get(keyword_type, keyword_type)
            
            print(f"   ‚Ä¢ {type_name}: {count} —Å–ª–æ–≤")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        print("\nüìù –ü–†–ò–ú–ï–†–´ –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í:")
        for group_type in GROUP_KEYWORDS:
            print(f"\nüè• {group_type.upper()}:")
            keywords = GROUP_KEYWORDS[group_type]
            
            for keyword_type, words in keywords.items():
                type_name = {
                    'name': '–ù–∞–∑–≤–∞–Ω–∏—è',
                    'text': '–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–æ–≤',
                    'negative': '–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ'
                }.get(keyword_type, keyword_type)
                
                print(f"   ‚Ä¢ {type_name}: {', '.join(words[:5])}{'...' if len(words) > 5 else ''}")
        
        conn.close()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=== –°–ò–°–¢–ï–ú–ê –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í –î–õ–Ø –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ì–†–£–ü–ü ===")
    print(f"‚è∞ –í—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤...")
    if create_keywords_table():
        print("‚úÖ –¢–∞–±–ª–∏—Ü–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å–æ–∑–¥–∞–Ω–∞")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã")
        return
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤:")
    show_keywords_statistics()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º detected_groups
    print("\nüîß –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ detected_groups...")
    if update_detected_groups():
        print("‚úÖ –ì—Ä—É–ø–ø—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥—Ä—É–ø–ø")
    
    print("\nüéâ –°–∏—Å—Ç–µ–º–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞!")

if __name__ == "__main__":
    main() 