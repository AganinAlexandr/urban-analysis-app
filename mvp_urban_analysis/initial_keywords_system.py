#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —É–ª—É—á—à–µ–Ω–∏–µ–º
"""
import sqlite3
import os
import re
from collections import Counter, defaultdict
from datetime import datetime

# –ù–∞—á–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
INITIAL_KEYWORDS = {
    'hospitals': {
        'name_keywords': ['–±–æ–ª—å–Ω–∏—Ü–∞', '–≥–æ—Å–ø–∏—Ç–∞–ª—å', '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä', '–∫–ª–∏–Ω–∏–∫–∞', '–º–µ–¥—Ü–µ–Ω—Ç—Ä', '–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞'],
        'text_keywords': ['–≤—Ä–∞—á', '–ª–µ—á–µ–Ω–∏–µ', '–ø–∞—Ü–∏–µ–Ω—Ç', '–º–µ–¥–∏—Ü–∏–Ω–∞', '–∑–¥–æ—Ä–æ–≤—å–µ', '–±–æ–ª–µ–∑–Ω—å', '—Å–∏–º–ø—Ç–æ–º', '–¥–∏–∞–≥–Ω–æ–∑', '–æ–ø–µ—Ä–∞—Ü–∏—è', '—Ç–µ—Ä–∞–ø–∏—è'],
        'negative_keywords': ['–æ—á–µ—Ä–µ–¥—å', '–∑–∞–ø–∏—Å—å', '–ø—Ä–∏–µ–º', '–∞–Ω–∞–ª–∏–∑', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ']
    },
    'schools': {
        'name_keywords': ['—à–∫–æ–ª–∞', '–ª–∏—Ü–µ–π', '–≥–∏–º–Ω–∞–∑–∏—è', '–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä', '—É—á–µ–±–Ω–æ–µ –∑–∞–≤–µ–¥–µ–Ω–∏–µ'],
        'text_keywords': ['—É—á–∏—Ç–µ–ª—å', '—É—á–µ–Ω–∏–∫', '—É—Ä–æ–∫', '–∫–ª–∞—Å—Å', '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–æ–±—É—á–µ–Ω–∏–µ', '–¥–∏—Ä–µ–∫—Ç–æ—Ä', '–∑–∞–≤—É—á', '–ø—Ä–µ–¥–º–µ—Ç', '—ç–∫–∑–∞–º–µ–Ω'],
        'negative_keywords': ['–¥–æ–º–∞—à–∫–∞', '–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è', '–æ—Ü–µ–Ω–∫–∞', '–¥–≤–æ–π–∫–∞', '—Ç—Ä–æ–π–∫–∞', '—á–µ—Ç–≤–µ—Ä–∫–∞', '–ø—è—Ç–µ—Ä–∫–∞']
    },
    'kindergartens': {
        'name_keywords': ['–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥', '—Å–∞–¥', '–¥–æ—à–∫–æ–ª—å–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ', '—è—Å–ª–∏', '–¥–µ—Ç—Å–∞–¥'],
        'text_keywords': ['–≤–æ—Å–ø–∏—Ç–∞—Ç–µ–ª—å', '—Ä–µ–±–µ–Ω–æ–∫', '–≥—Ä—É–ø–ø–∞', '–∏–≥—Ä–∞', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '–∑–∞–Ω—è—Ç–∏–µ', '–ø—Ä–æ–≥—É–ª–∫–∞', '—Å–æ–Ω', '–µ–¥–∞', '–∞–¥–∞–ø—Ç–∞—Ü–∏—è'],
        'negative_keywords': ['–ø–ª–∞—á', '–∫–∞–ø—Ä–∏–∑', '–Ω–µ —Ö–æ—á—É', '–Ω–µ –±—É–¥—É', '—Å—Ç—Ä–∞—à–Ω–æ', '–±–æ—é—Å—å']
    },
    'polyclinics': {
        'name_keywords': ['–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞', '–∞–º–±—É–ª–∞—Ç–æ—Ä–∏—è', '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', '–¥–∏—Å–ø–∞–Ω—Å–µ—Ä'],
        'text_keywords': ['—Ç–µ—Ä–∞–ø–µ–≤—Ç', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', '–æ—Å–º–æ—Ç—Ä', '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '—Å–ø—Ä–∞–≤–∫–∞', '–±–æ–ª—å–Ω–∏—á–Ω—ã–π', '—Ä–µ—Ü–µ–ø—Ç'],
        'negative_keywords': ['–æ—á–µ—Ä–µ–¥—å', '–∑–∞–ø–∏—Å—å', '–ø—Ä–∏–µ–º', '–∞–Ω–∞–ª–∏–∑', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ']
    },
    'pharmacies': {
        'name_keywords': ['–∞–ø—Ç–µ–∫–∞', '—Ñ–∞—Ä–º–∞—Ü–∏—è', '–ª–µ–∫–∞—Ä—Å—Ç–≤–æ', '–º–µ–¥–∏–∫–∞–º–µ–Ω—Ç'],
        'text_keywords': ['–ª–µ–∫–∞—Ä—Å—Ç–≤–æ', '—Ç–∞–±–ª–µ—Ç–∫–∞', '—Å–∏—Ä–æ–ø', '–º–∞–∑—å', '—Ä–µ—Ü–µ–ø—Ç', '—Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç', '–ø—Ä–æ–≤–∏–∑–æ—Ä', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–∞–Ω–∞–ª–æ–≥'],
        'negative_keywords': ['–¥–æ—Ä–æ–≥–æ', '–Ω–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏', '–∑–∞–º–µ–Ω–∏—Ç–µ–ª—å', '–ø–æ–±–æ—á–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç']
    },
    'shopping_malls': {
        'name_keywords': ['—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä', '–º–æ–ª–ª', '–≥–∞–ª–µ—Ä–µ—è', '–ø–∞—Å—Å–∞–∂', '—Ç–æ—Ä–≥–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å', '—à–æ–ø–ø–∏–Ω–≥', '—Ç—Ü'],
        'text_keywords': ['–º–∞–≥–∞–∑–∏–Ω', '–ø–æ–∫—É–ø–∫–∞', '—Ç–æ–≤–∞—Ä', '—Ü–µ–Ω–∞', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '–∫–∞—Å—Å–∞', '–ø—Ä–æ–¥–∞–≤–µ—Ü', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç', '—Ä–∞–∑–º–µ—Ä'],
        'negative_keywords': ['–¥–æ—Ä–æ–≥–æ', '–æ—á–µ—Ä–µ–¥—å', '–Ω–µ—Ç —Ä–∞–∑–º–µ—Ä–∞', '–Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç', '–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è']
    },
    'universities': {
        'name_keywords': ['—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '–∏–Ω—Å—Ç–∏—Ç—É—Ç', '–∞–∫–∞–¥–µ–º–∏—è', '–≤—É–∑', '–≤—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ'],
        'text_keywords': ['—Å—Ç—É–¥–µ–Ω—Ç', '–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å', '–ª–µ–∫—Ü–∏—è', '—Å–µ–º–∏–Ω–∞—Ä', '—ç–∫–∑–∞–º–µ–Ω', '—Å–µ—Å—Å–∏—è', '–¥–∏–ø–ª–æ–º', '–∫–∞—Ñ–µ–¥—Ä–∞', '—Ñ–∞–∫—É–ª—å—Ç–µ—Ç', '—Ä–µ–∫—Ç–æ—Ä', '–ª–µ–∫—Ç–æ—Ä'],
        'negative_keywords': ['—Å–ª–æ–∂–Ω–æ', '—Ç—Ä—É–¥–Ω–æ', '–Ω–µ –ø–æ–Ω–∏–º–∞—é', '–∑–∞–≤–∞–ª–∏–ª', '–Ω–µ —Å–¥–∞–ª']
    }
}

class InitialKeywordProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —É–ª—É—á—à–µ–Ω–∏—è"""
    
    def __init__(self):
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.normalization_dict = {
            '–±–æ–ª—å–Ω–∏—Ü–∞': '–±–æ–ª—å–Ω–∏—Ü–∞',
            '–≥–æ—Å–ø–∏—Ç–∞–ª—å': '–±–æ–ª—å–Ω–∏—Ü–∞',
            '–º–µ–¥—Ü–µ–Ω—Ç—Ä': '–±–æ–ª—å–Ω–∏—Ü–∞',
            '–∫–ª–∏–Ω–∏–∫–∞': '–±–æ–ª—å–Ω–∏—Ü–∞',
            '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π': '–±–æ–ª—å–Ω–∏—Ü–∞',
            '–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞': '–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞',
            '–∞–º–±—É–ª–∞—Ç–æ—Ä–∏—è': '–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞',
            '—à–∫–æ–ª–∞': '—à–∫–æ–ª–∞',
            '–ª–∏—Ü–µ–π': '—à–∫–æ–ª–∞',
            '–≥–∏–º–Ω–∞–∑–∏—è': '—à–∫–æ–ª–∞',
            '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç': '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç',
            '–∏–Ω—Å—Ç–∏—Ç—É—Ç': '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç',
            '–∞–∫–∞–¥–µ–º–∏—è': '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç',
            '–≤—É–∑': '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç',
            '–∞–ø—Ç–µ–∫–∞': '–∞–ø—Ç–µ–∫–∞',
            '—Ñ–∞—Ä–º–∞—Ü–∏—è': '–∞–ø—Ç–µ–∫–∞',
            '–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥': '–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥',
            '—Å–∞–¥': '–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥',
            '–¥–µ—Ç—Å–∞–¥': '–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥',
            '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä': '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä',
            '–º–æ–ª–ª': '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä',
            '–≥–∞–ª–µ—Ä–µ—è': '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä',
            '–º–∞–≥–∞–∑–∏–Ω': '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä',
            '—Ç—Ü': '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä'
        }
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        self.stop_words = {
            '—ç—Ç–æ', '—Ç–æ—Ç', '—Ç–∞–∫–æ–π', '–∫–∞–∫–æ–π', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫–∞–∫', '—á—Ç–æ', '–∫—Ç–æ',
            '–æ—á–µ–Ω—å', '–º–Ω–æ–≥–æ', '–º–∞–ª–æ', '–±–æ–ª—å—à–µ', '–º–µ–Ω—å—à–µ', '–≤—Å–µ', '–≤—Å–µ–≥–¥–∞', '–Ω–∏–∫–æ–≥–¥–∞',
            '—Ö–æ—Ä–æ—à–æ', '–ø–ª–æ—Ö–æ', '–ª—É—á—à–µ', '—Ö—É–∂–µ', '–Ω–æ–≤—ã–π', '—Å—Ç–∞—Ä—ã–π', '–±–æ–ª—å—à–æ–π', '–º–∞–ª–µ–Ω—å–∫–∏–π',
            '–µ—Å—Ç—å', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏', '–±—ã—Ç—å', '—Å—Ç–∞—Ç—å', '—Å—Ç–∞–ª', '—Å—Ç–∞–ª–∞'
        }
    
    def normalize_word(self, word):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ª–æ–≤–æ, –ø—Ä–∏–≤–æ–¥—è –∫ –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º–µ"""
        if not word:
            return ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        word = word.lower().strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–æ–≤–∞—Ä—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        if word in self.normalization_dict:
            return self.normalization_dict[word]
        
        # –ü—Ä–æ—Å—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ–∫–æ–Ω—á–∞–Ω–∏–π
        if word.endswith('–∞—è'):
            word = word[:-2] + '–∞—è'
        elif word.endswith('—ã–π'):
            word = word[:-2] + '—ã–π'
        elif word.endswith('–æ–π'):
            word = word[:-2] + '–æ–π'
        elif word.endswith('–∏–π'):
            word = word[:-2] + '–∏–π'
        
        return word
    
    def clean_text(self, text):
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not text:
            return ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ü–∏—Ñ—Ä—ã
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_normalized_words(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return []
        
        cleaned_text = self.clean_text(text)
        words = cleaned_text.split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        normalized_words = []
        for word in words:
            if len(word) >= 3 and word not in self.stop_words:
                normalized_word = self.normalize_word(word)
                if normalized_word and len(normalized_word) >= 3:
                    normalized_words.append(normalized_word)
        
        return normalized_words

def create_initial_keywords_table():
    """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS initial_keywords (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                group_type TEXT NOT NULL,
                keyword_type TEXT NOT NULL,
                keyword TEXT NOT NULL,
                weight REAL DEFAULT 1.0,
                is_initial BOOLEAN DEFAULT 1,
                frequency INTEGER DEFAULT 1,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(group_type, keyword_type, keyword)
            )
        """)
        
        # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        cursor.execute("DELETE FROM initial_keywords")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        added_count = 0
        for group_type, keywords in INITIAL_KEYWORDS.items():
            for keyword_type, words in keywords.items():
                for word in words:
                    cursor.execute("""
                        INSERT INTO initial_keywords 
                        (group_type, keyword_type, keyword, weight, is_initial, frequency)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (group_type, keyword_type, word, 1.0, 1, 1))
                    added_count += 1
        
        conn.commit()
        conn.close()
        
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {added_count}")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã: {e}")
        return False

def detect_group_by_initial_keywords(object_name, review_text):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä—É–ø–ø—É –æ–±—ä–µ–∫—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return 'undetected', 0.0
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        cursor.execute("""
            SELECT group_type, keyword_type, keyword, weight
            FROM initial_keywords
            ORDER BY group_type, keyword_type, keyword
        """)
        
        keywords = cursor.fetchall()
        
        if not keywords:
            print("‚ùå –ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
            conn.close()
            return 'undetected', 0.0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        processor = InitialKeywordProcessor()
        normalized_object_name = processor.clean_text(object_name or "")
        normalized_review_text = processor.clean_text(review_text or "")
        
        print(f"üîç –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–∫—Ç–∞: '{object_name}'")
        print(f"üìù –û—Ç–∑—ã–≤: '{review_text}'")
        print(f"üîß –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: '{normalized_object_name}'")
        print(f"üîß –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–∑—ã–≤: '{normalized_review_text}'")
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –±–∞–ª–ª—ã –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        group_scores = defaultdict(lambda: {'name_score': 0, 'text_score': 0, 'negative_score': 0})
        
        for group_type, keyword_type, keyword, weight in keywords:
            normalized_keyword = processor.clean_text(keyword)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞
            if keyword_type == 'name' and normalized_object_name:
                if normalized_keyword in normalized_object_name:
                    group_scores[group_type]['name_score'] += weight
                    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏: '{normalized_keyword}' –≤ '{normalized_object_name}'")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ –æ—Ç–∑—ã–≤–∞
            if keyword_type == 'text' and normalized_review_text:
                if normalized_keyword in normalized_review_text:
                    group_scores[group_type]['text_score'] += weight
                    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ –æ—Ç–∑—ã–≤–µ: '{normalized_keyword}' –≤ '{normalized_review_text}'")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if keyword_type == 'negative':
                if normalized_object_name and normalized_keyword in normalized_object_name:
                    group_scores[group_type]['negative_score'] += weight
                    print(f"   ‚ùå –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏: '{normalized_keyword}' –≤ '{normalized_object_name}'")
                if normalized_review_text and normalized_keyword in normalized_review_text:
                    group_scores[group_type]['negative_score'] += weight
                    print(f"   ‚ùå –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –≤ –æ—Ç–∑—ã–≤–µ: '{normalized_keyword}' –≤ '{normalized_review_text}'")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            if keyword_type == 'text' and not keyword.startswith('text_'):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                if normalized_object_name and normalized_keyword in normalized_object_name:
                    group_scores[group_type]['name_score'] += weight * 0.5
                    print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–Ω–æ–µ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏: '{normalized_keyword}' –≤ '{normalized_object_name}'")
                if normalized_review_text and normalized_keyword in normalized_review_text:
                    group_scores[group_type]['text_score'] += weight
                    print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–Ω–æ–µ –≤ –æ—Ç–∑—ã–≤–µ: '{normalized_keyword}' –≤ '{normalized_review_text}'")
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        print(f"\nüîç –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
        print(f"   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: '{normalized_object_name}'")
        print(f"   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–∑—ã–≤: '{normalized_review_text}'")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(keywords)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        print(f"   –ü—Ä–∏–º–µ—Ä—ã –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤:")
        for i, (group_type, keyword_type, keyword, weight) in enumerate(keywords[:5]):
            normalized_keyword = processor.clean_text(keyword)
            print(f"      {i+1}. {group_type}.{keyword_type}: '{keyword}' -> '{normalized_keyword}'")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        best_group = 'undetected'
        best_score = 0.0
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—Å—á–µ—Ç–∞ –±–∞–ª–ª–æ–≤:")
        for group_type, scores in group_scores.items():
            # –§–æ—Ä–º—É–ª–∞: (name_score * 2) + text_score - negative_score
            total_score = (scores['name_score'] * 2) + scores['text_score'] - scores['negative_score']
            
            print(f"   {group_type}: name={scores['name_score']}, text={scores['text_score']}, negative={scores['negative_score']}, total={total_score}")
            
            if total_score > best_score:
                best_score = total_score
                best_group = group_type
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0.0 - 1.0)
        confidence = min(best_score / 5.0, 1.0) if best_score > 0 else 0.0
        
        conn.close()
        return best_group, confidence
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä—É–ø–ø—ã: {e}")
        return 'undetected', 0.0

def update_initial_keywords_from_data():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("üîÑ –û–ë–ù–û–í–õ–ï–ù–ò–ï –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í –ù–ê –û–°–ù–û–í–ï –î–ê–ù–ù–´–•")
    print("=" * 60)
    
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã —Å –∏—Ö –≥—Ä—É–ø–ø–∞–º–∏ –∏ –æ—Ç–∑—ã–≤–∞–º–∏
        cursor.execute("""
            SELECT 
                og.group_type,
                o.name,
                GROUP_CONCAT(r.review_text, ' | ') as all_reviews
            FROM objects o
            LEFT JOIN object_groups og ON o.group_id = og.id
            LEFT JOIN reviews r ON o.id = r.object_id
            WHERE og.group_type IS NOT NULL
            GROUP BY og.group_type, o.name
        """)
        
        group_data = defaultdict(lambda: {'names': [], 'reviews': []})
        
        for group_type, name, reviews_text in cursor.fetchall():
            if group_type:
                group_data[group_type]['names'].append(name)
                if reviews_text:
                    group_data[group_type]['reviews'].append(reviews_text)
        
        if not group_data:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
            conn.close()
            return False
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        processor = InitialKeywordProcessor()
        group_words = {}
        all_words = Counter()
        
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø–æ –≥—Ä—É–ø–ø–∞–º:")
        for group_type, data in group_data.items():
            print(f"\nüè• {group_type.upper()}:")
            print(f"   üìÑ –ù–∞–∑–≤–∞–Ω–∏–π –æ–±—ä–µ–∫—Ç–æ–≤: {len(data['names'])}")
            print(f"   üìù –û—Ç–∑—ã–≤–æ–≤: {len(data['reviews'])}")
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏–π
            name_words = []
            for name in data['names']:
                name_words.extend(processor.extract_normalized_words(name))
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –æ—Ç–∑—ã–≤–æ–≤
            review_words = []
            for reviews_text in data['reviews']:
                review_words.extend(processor.extract_normalized_words(reviews_text))
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤
            name_word_counts = Counter(name_words)
            review_word_counts = Counter(review_words)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –¥–ª—è –≥—Ä—É–ø–ø—ã
            group_words[group_type] = {
                'name_words': name_word_counts,
                'review_words': review_word_counts,
                'all_words': name_word_counts + review_word_counts
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å—á–µ—Ç—á–∏–∫
            all_words.update(name_word_counts)
            all_words.update(review_word_counts)
            
            print(f"   üî§ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö: {len(name_word_counts)}")
            print(f"   üî§ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –≤ –æ—Ç–∑—ã–≤–∞—Ö: {len(review_word_counts)}")
        
        # –ù–∞—Ö–æ–¥–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        unique_keywords = {}
        min_frequency = 2
        
        print(f"\nüîç –ü–æ–∏—Å–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–º–∏–Ω. —á–∞—Å—Ç–æ—Ç–∞: {min_frequency}):")
        
        for group_type, words_data in group_words.items():
            print(f"\nüè• {group_type.upper()}:")
            
            # –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ —ç—Ç–æ–π –≥—Ä—É–ø–ø–µ
            group_unique_words = []
            
            for word, count in words_data['all_words'].items():
                if count >= min_frequency:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ª–æ–≤–æ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –¥—Ä—É–≥–∏—Ö –≥—Ä—É–ø–ø–∞—Ö
                    is_unique = True
                    for other_group, other_words_data in group_words.items():
                        if other_group != group_type:
                            if word in other_words_data['all_words']:
                                is_unique = False
                                break
                    
                    if is_unique:
                        group_unique_words.append((word, count))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
            group_unique_words.sort(key=lambda x: x[1], reverse=True)
            
            # –ë–µ—Ä–µ–º —Ç–æ–ø-10 —Å–ª–æ–≤
            unique_keywords[group_type] = group_unique_words[:10]
            
            print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(group_unique_words)}")
            for word, count in group_unique_words[:5]:
                print(f"      ‚Ä¢ '{word}' (—á–∞—Å—Ç–æ—Ç–∞: {count})")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        if unique_keywords:
            print(f"\nüíæ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–Ω–µ —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ)
            added_count = 0
            for group_type, words in unique_keywords.items():
                for word, frequency in words:
                    try:
                        cursor.execute("""
                            INSERT INTO initial_keywords 
                            (group_type, keyword_type, keyword, weight, is_initial, frequency)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (group_type, 'text', word, 1.0, 0, frequency))
                        added_count += 1
                    except sqlite3.IntegrityError:
                        # –°–ª–æ–≤–æ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ–±–Ω–æ–≤–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—É
                        cursor.execute("""
                            UPDATE initial_keywords 
                            SET frequency = ?, updated_at = CURRENT_TIMESTAMP
                            WHERE group_type = ? AND keyword_type = ? AND keyword = ?
                        """, (frequency, group_type, 'text', word))
            
            conn.commit()
            print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ/–æ–±–Ω–æ–≤–ª–µ–Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {added_count}")
        else:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        
        conn.close()
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        return False

def show_initial_keywords_status():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    db_path = 'urban_analysis_fixed.db'
    
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{db_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        print("üìä –°–¢–ê–¢–£–° –ù–ê–ß–ê–õ–¨–ù–´–• –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í:")
        print("=" * 50)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º
        cursor.execute("""
            SELECT group_type, keyword_type, is_initial, COUNT(*) as count
            FROM initial_keywords
            GROUP BY group_type, keyword_type, is_initial
            ORDER BY group_type, keyword_type, is_initial
        """)
        
        stats = cursor.fetchall()
        
        current_group = None
        for group_type, keyword_type, is_initial, count in stats:
            if current_group != group_type:
                print(f"\nüìÑ {group_type.upper()}:")
                current_group = group_type
            
            type_name = {
                'name': '–ù–∞–∑–≤–∞–Ω–∏—è',
                'text': '–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–æ–≤', 
                'negative': '–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ'
            }.get(keyword_type, keyword_type)
            
            source = "–ù–∞—á–∞–ª—å–Ω—ã–µ" if is_initial else "–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ"
            print(f"   ‚Ä¢ {type_name} ({source}): {count} —Å–ª–æ–≤")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        print("\nüìù –ü–†–ò–ú–ï–†–´ –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í:")
        for group_type in INITIAL_KEYWORDS:
            print(f"\nüè• {group_type.upper()}:")
            
            cursor.execute("""
                SELECT keyword_type, keyword, is_initial, frequency
                FROM initial_keywords
                WHERE group_type = ?
                ORDER BY keyword_type, is_initial DESC, frequency DESC
                LIMIT 5
            """, (group_type,))
            
            keywords = cursor.fetchall()
            
            for keyword_type, keyword, is_initial, frequency in keywords:
                type_name = {
                    'name': '–ù–∞–∑–≤–∞–Ω–∏—è',
                    'text': '–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–æ–≤',
                    'negative': '–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ'
                }.get(keyword_type, keyword_type)
                
                source = "–ù–∞—á–∞–ª—å–Ω—ã–µ" if is_initial else "–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ"
                print(f"   ‚Ä¢ {type_name} ({source}): '{keyword}' (—á–∞—Å—Ç–æ—Ç–∞: {frequency})")
        
        conn.close()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

def test_initial_system():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–∏—Å—Ç–µ–º—É –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    print("=== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–ò–°–¢–ï–ú–´ –ù–ê–ß–ê–õ–¨–ù–´–• –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í ===")
    print(f"‚è∞ –í—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    test_cases = [
        ("–ì–æ—Ä–æ–¥—Å–∫–∞—è –±–æ–ª—å–Ω–∏—Ü–∞ ‚Ññ1", "–û—Ç–ª–∏—á–Ω–∞—è –±–æ–ª—å–Ω–∏—Ü–∞ —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –≤—Ä–∞—á–∞–º–∏"),
        ("–ê–ø—Ç–µ–∫–∞ –Ω–∞ —É–≥–ª—É", "–£–¥–æ–±–Ω–∞—è –∞–ø—Ç–µ–∫–∞, –≤—Å–µ–≥–¥–∞ –µ—Å—Ç—å –Ω—É–∂–Ω—ã–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞"),
        ("–®–∫–æ–ª–∞ ‚Ññ15", "–•–æ—Ä–æ—à–∞—è —à–∫–æ–ª–∞, –Ω–æ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–∏—Ç–∞–Ω–∏–µ–º"),
        ("–î–µ—Ç—Å–∫–∏–π —Å–∞–¥ –°–æ–ª–Ω—ã—à–∫–æ", "–í–æ—Å–ø–∏—Ç–∞—Ç–µ–ª–∏ –æ—á–µ–Ω—å –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–µ –∫ –¥–µ—Ç—è–º"),
        ("–¢–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä –ú–µ–≥–∞", "–ë–æ–ª—å—à–æ–π –≤—ã–±–æ—Ä –º–∞–≥–∞–∑–∏–Ω–æ–≤ –∏ —Ç–æ–≤–∞—Ä–æ–≤"),
        ("–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –ú–ì–£", "–°–ª–æ–∂–Ω—ã–µ —ç–∫–∑–∞–º–µ–Ω—ã, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ")
    ]
    
    for i, (object_name, review_text) in enumerate(test_cases, 1):
        print(f"\nüß™ –¢–µ—Å—Ç {i}:")
        print(f"üìÑ –û–±—ä–µ–∫—Ç: '{object_name}'")
        print(f"üìù –û—Ç–∑—ã–≤: '{review_text}'")
        
        detected_group, confidence = detect_group_by_initial_keywords(object_name, review_text)
        
        print(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç: {detected_group} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
        print("-" * 50)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=== –°–ò–°–¢–ï–ú–ê –ù–ê–ß–ê–õ–¨–ù–´–• –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í ===")
    print(f"‚è∞ –í—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤...")
    if not create_initial_keywords_table():
        print("‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã")
        return
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å
    print("\nüìä –°—Ç–∞—Ç—É—Å –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤:")
    show_initial_keywords_status()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤...")
    test_initial_system()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –µ—Å—Ç—å)
    print("\nüîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö...")
    if update_initial_keywords_from_data():
        print("‚úÖ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –æ–±–Ω–æ–≤–ª–µ–Ω—ã")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å
        print("\nüìä –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤:")
        show_initial_keywords_status()
    else:
        print("‚ÑπÔ∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
    
    print("\nüéâ –°–∏—Å—Ç–µ–º–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞!")

if __name__ == "__main__":
    main() 